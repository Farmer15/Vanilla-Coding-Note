# 딥러닝 모델 → 그래픽카드 관련 정리

### 딥러닝 모델 불러오면서 마주친 문제

https://huggingface.co/Qwen/Qwen2-Math-7B-Instruct

- 초기 로드시
    - “ou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.”
        
        ![image.png](../%E1%84%8C%E1%85%A6%E1%84%86%E1%85%A9%E1%86%A8%20%E1%84%8B%E1%85%A5%E1%86%B9%E1%84%8B%E1%85%B3%E1%86%B7%2035d1add0447a492f81bc2b4821e80635/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%2010a7604d886e80d3b4bed3d0d7de907b/image.png)
        
        → 모델을 로드할 메모리가 충분하지 않을 때 발생하기 떄문에 `disk_offload` 를 사용해서 해결해야합니다. 
        
        ```python
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype="auto", device_map="auto"
        )
        
        ----------------------------------------------------------------
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype="auto", low_cpu_mem_usage=True
        ).cpu()
        
        disk_offload(model=model, offload_dir="offload")
        ```
        
        → 참고자료 : https://medium.com/@sridevi17j/resolving-valueerror-you-are-trying-to-offload-the-whole-model-to-the-disk-70d4e8138797
        
    - “AssertionError: Torch not compiled with CUDA enabled”
        
        →M2 mac 에서는 Nvidia GPU 기능 CUDA 가 없기 때문에 사용할 수 없는 문제가 발생했습니다. 
        
        ![image.png](../%E1%84%8C%E1%85%A6%E1%84%86%E1%85%A9%E1%86%A8%20%E1%84%8B%E1%85%A5%E1%86%B9%E1%84%8B%E1%85%B3%E1%86%B7%2035d1add0447a492f81bc2b4821e80635/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%2010a7604d886e80d3b4bed3d0d7de907b/image%201.png)
        
        - 해결
            
            https://big-data-analyst.tistory.com/19
            
            ![image.png](../%E1%84%8C%E1%85%A6%E1%84%86%E1%85%A9%E1%86%A8%20%E1%84%8B%E1%85%A5%E1%86%B9%E1%84%8B%E1%85%B3%E1%86%B7%2035d1add0447a492f81bc2b4821e80635/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%2010a7604d886e80d3b4bed3d0d7de907b/image%202.png)
            
            ```python
            device = "cuda"
            ------------------------------------------------------------
            device = "mps"
            ```
            
    - “indices should be either on cpu or on the same device as the indexed tensor (cpu)”
        
        → 서로 다른 환경에서 실행되고 있어서 한 쪽으로 일치 시켜주어야 합니다. 
        
        ```python
        device = "mps"
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto"
            low_cpu_mem_usage=True,     
        ).to(device)
        
        model_inputs = tokenizer([text], return_tensors="pt").to(device)
        ```
        
    - “RuntimeError: MPS backend out of memory (MPS allocated: 8.97 GB, other allocations: 384.00 KB, max allowed: 9.07 GB). Tried to allocate 129.50 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).”
        
        ![image.png](../%E1%84%8C%E1%85%A6%E1%84%86%E1%85%A9%E1%86%A8%20%E1%84%8B%E1%85%A5%E1%86%B9%E1%84%8B%E1%85%B3%E1%86%B7%2035d1add0447a492f81bc2b4821e80635/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%2010a7604d886e80d3b4bed3d0d7de907b/image%203.png)
        
        → MPS 에서 사용 가능한 메모리 공간이 부족할 때뜨는 오류 메세지로 모델이 너무 크거나 한번에 처리하려는 데이터가 많을때 발생하는 문제입니다.
        
        ```python
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16     # "auto" --> 더작은 메모리 사용하는 "float16" 변경
            low_cpu_mem_usage=True,     
        ).to(device)
        
        generated_ids = model.generate(**model_inputs, max_new_tokens=256)  #한번에 처리하는 512 --> 256 으로 변경
        ```
        
        → 그렇게 사용해도 똑같은 오류 나오는 것을 확인 해당 딥러닝 모델 낮은버젼을 사용 
        
        https://huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm