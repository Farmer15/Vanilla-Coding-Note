# 자연어 처리

날짜: 2024년 9월 19일

### 자연어 처리 (NLP) 란 ?

→ 인간의 언어와 표현을 컴퓨터가 처리할 수 있도록 하는 계산 기법을 말합니다.

### 과정

1. 토큰화
    - 의미를 가지는 작은 단위로 분리합니다.
        
        ```python
        from nltk.tokenize import word_tokenize
        
        text = "Although it's not a happily-ever-after ending, it is very realistic."
        
        tokenized_words = word_tokenize(text)
        
        print(tokenized_words)            # ['Although', 'it', "'s", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']
        ```
        
    - 문장 토큰화
        
        ```python
        from nltk.tokenize import sent_tokenize
        
        nltk.download('punkt')
        
        text = "My email address is 'abcde@codeit.com'. Send it to Mr.Kim."
        
        # 문장 토큰화
        tokenized_sents = sent_tokenize(text)
        
        ```
        
    - 품사 태깅
        
        → 토큰화 된 각 토큰에 품사들을 표시해주는 역할
        
        ```python
        from nltk.tag import pos_tag
        from nltk.tokenize import word_tokenize
        
        text = "Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \"Hey, let\'s pool our money together and make a really bad movie!\" Or something like that."
        pos_tagged_words = []
        
        tokenized_words = word_tokenize(text)
        
        pos_tagged = pos_tag(tokenized_words)
        pos_tagged_words.extend(pos_tagged)
        ```
        
        ![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image.png)
        
2. 정제
    
    아무의미 없거나 목적에 부합하지 않는 단어를 전처리 단계에서 삭제하는 과정 
    
    - 등장 빈도가 낮은 단어 제거
        
        ```python
        from collections import Counter
        
        tokenized_words = word_tokenize(Text)
        
        vocab = Counter(tokenized_words)      # 단어 개수를 카운트 해줌 { 무슨단어: n개 , 무슨단어: n개 }
        
        uncommon_words = [key for key, value in vocab.items() if value <= 2]  # .items() --> 객체를 튜프 배열로 바꿔줌
        
        cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]
        ```
        
    - 특정 길이 이하 제거
        
        ```python
        cleaned_by_freq_len = []
        
        for word in cleaned_by_freq:
            if len(word) > 2:
                cleaned_by_freq_len.append(word)
        ```
        
    - 불용어 제거
        
        ```python
        from nltk.corpus import stopwords
        
        nltk.download("stopwords")
        
        stopwords_set = set(stopwords.words("english"))     # 불용어 모음
        
        stopwords_set.add("hello")    # 불용어 추가
        
        stopwords_set.remove("the")     # 불용어 삭제
        
        cleaned_words = []
        
        for word in cleaned_by_freq_len:
            if word not in stop_words_set:
                cleaned_words.append(word)
        ```
        
        - 내가 임의로 불용어 셋트 정의 가능
        
        ```python
        my_stopwords_set = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves'}
        ```
        
3. 추출
    - 어간 추출
        
        → 어간(특정단어의 핵심이 되는 부분)을 찾아서 추출해주는 방식을 나타냅니다. (activate —> active, lovely —> love )
        
        ```python
        from nltk.stem import PorterStemmer
        
        porter_stemmer = PorterStemmer()
        
        text = "You are so lovely. I am loving you now."
        porter_stemmed_words = []
        
        tokenized_words = nltk.word_tokenize(text)      # 문장을 토큰화 시켜주기
        
        for word in tokenized_words:                    # 어간 추출 반복문
            stem = porter_stemmer.stem(word)            # 매개변수 단어가 어간이면 그대로 반환 아니면 어간으로 바뀌어서 반환
            porter_stemmed_words.append(stem) 
        ```
        
    
    - 표제어 추출
        1. 품사를 기준으로 표제어를 추출하기 때문에 먼져 품사 태깅을 해주어야 합니다.
        
        ```python
        from nltk.tag import pos_tag
        from nltk.tokenize import word_tokenize
        
        text = "You are the happiest person."
        tokenized_words = word_tokenize(text)
        
        # 품사 태그
        tagged_words = pos_tag(tokenized_words)
        ```
        
        1. 각 품사 태깅된 토큰들을 WordNet POS Tag 에 맞게 변환 시켜주어야 합니다.
            - wn.NOUN(n) : 명사
            - wn.ADJ(a) : 형용사
            - wn.ADV(r) : 부사
            - wn.VERB(v) : 동사
            
        2. 여기서 문제가 토큰에 태깅된 품사들은 종류가 더 다양합니다.
            - CC, CD, DT, EX, FW, IN, JJ, JJR, JJS, LS, MD, NN ….
            
            → 규칙 
            
            - N으로 시작하는 태그는 모두 명사(Noun)를 의미
            - J로 시작하는 태그는 모두 형용사(Adjective)를 의미
            - R로 시작하는 태그는 모두 부사(Adverb)를 의미
            - V로 시작하는 태그는 모두 동사(Verb)를 의미
            
        
        ```python
        from nltk.corpus import wordnet as wn
        
        def penn_to_wn(tag):
            if tag.startswith('J'):
                return wn.ADJ
            elif tag.startswith('N'):
                return wn.NOUN
            elif tag.startswith('R'):
                return wn.ADV
            elif tag.startswith('V'):
                return wn.VERB
            else:
                return
                
        for word, tag in tagged_words:
        
          wn_tag = penn_to_wn(tag)
        
          if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):
              lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))
          else:
              lemmatized_words.append(word)
        ```
        
4. 통합하기
    
    → 그렇게 추출까지 마친 토큰들을 하나로 통합해주어야 합니다. 
    
    → **원하는 형태로 이어붙이기게 로직을 구성하면 됩니다.**
    
5. 인코딩
    
    → 자연어를 컴퓨터가 처리하도록 숫자로 바꾸는 과정을 나타냅니다. (컴퓨터는 텍스트보다 숫자를 더 잘 처리 할 수 있기 떄문)
    
    - 정수 인코딩(**Integer Encoding**)
        - 단어 빈도수 순으로 정렬한 단어 집합을 만들고 ({ “단어” : n번  }) 빈도수 높은 순서부터 낮은 순으로 정수를 부여하는 방식
            
            
            ![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image%201.png)
            
            - 개수 세어서
                
                ![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image%202.png)
                
            - 차례 대로 정수를 부여하고
                
                ![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image%203.png)
                
            - 해당 토큰들을 해당 정수로 바꾸어 줍니다.
                
                ![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image%204.png)
                
            - 전체에 적용
                
                ![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image%205.png)
                
    - 원-핫 인코딩(**One-Hot Encoding**)
        
        → 각 단어를 고유한 인덱스를 주고 차원에 따라 0, 1 로 표현하여 표현할 단어에 1 부여, 다른 인덱스에 0을 부여하는 벡터 표현 방식입니다. (가장 기본적인 표현 방법)
        
        ![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image%206.png)
        
        - 한계
            - 단어간의 유사도를 알 수 가 없습니다.
            - 단어가 늘어날때 마다
    - **TF-IDF (Term Frequency-Inverse Document Frequency)**
    - **워드 임베딩 (Word Embedding)**
        - 통계 기반으로 단어 빈도수를 기반으로 벡터로 표현하는 방식
        
    - **BERT와 같은 트랜스포머 기반 모델**
    - **Byte Pair Encoding (BPE)**
6. 마지막 우리가 원하는 작업

## 한글에 적용시 주의점

- 띄어쓰기 교정
    
    → 한국어에 대해서는 올바른 띄어쓰기가 되어 있어야 올바른 토큰화를 진행 할 수 있습니다.
    
    ```python
    from hanspell import spell_checker
    
    text = "아버지가방에들어가신다나는오늘코딩을했다"
    
    hanspell_sent = spell_checker.check(text)
    print(hanspell_sent.checked)
    ```
    
    ![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image%207.png)
    

![image.png](%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20f9e8f4de62074e1e95fdc6fb8547b431/image%208.png)